{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run those two cells before running the Notebook!\n",
    "\n",
    "As those plotting settings are standard throughout the book, we do not show them in the book every time we plot something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T10:48:06.120195Z",
     "start_time": "2020-01-29T10:48:05.814125Z"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T10:48:13.141309Z",
     "start_time": "2020-01-29T10:48:13.137453Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "# FIX: Use the official public API path from pandas.errors\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "# feel free to modify, for example, change the context to \"notebook\"\n",
    "sns.set_theme(context=\"talk\", style=\"whitegrid\", \n",
    "              palette=\"colorblind\", color_codes=True, \n",
    "              rc={\"figure.figsize\": [12, 8]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15 - Deep Learning in Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.1 Exploring `fastai`'s Tabular Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T13:25:43.412338Z",
     "start_time": "2020-01-17T13:25:41.274701Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIX: Install the fastai library\n",
    "!pip install fastai\n",
    "\n",
    "# Now your original imports will work\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from chapter_15_utils import performance_evaluation_report_fastai\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T23:47:19.038850Z",
     "start_time": "2019-12-23T23:47:19.036355Z"
    }
   },
   "source": [
    "2. Load the dataset from a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T13:25:43.675235Z",
     "start_time": "2020-01-17T13:25:43.535484Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/credit_card_default.csv\", \n",
    "                 na_values=\"\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a reminder, where the possible missing values are\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define the target, lists of categorical/numerical features, and the preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"default_payment_next_month\"\n",
    "\n",
    "cat_features = list(df.select_dtypes(\"object\").columns)\n",
    "num_features = list(df.select_dtypes(\"number\").columns)\n",
    "num_features.remove(TARGET)\n",
    "\n",
    "preprocessing = [FillMissing, Categorify, Normalize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Define the splitter used to create training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RandomSplitter(valid_pct=0.2, seed=42)(range_of(df))\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create the `TabularPandas` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_df = TabularPandas(\n",
    "    df, \n",
    "    procs=preprocessing,\n",
    "    cat_names=cat_features,\n",
    "    cont_names=num_features,\n",
    "    y_names=TARGET,\n",
    "    y_block=CategoryBlock(),\n",
    "    splits=splits\n",
    ")\n",
    "\n",
    "PREVIEW_COLS = [\"sex\", \"education\", \"marriage\", \n",
    "                \"payment_status_sep\", \"age_na\", \"limit_bal\",\n",
    "                \"age\", \"bill_statement_sep\"]\n",
    "tabular_df.xs.iloc[:5][PREVIEW_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_df.xs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define a `DataLoaders` object from the `TabularPandas` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = tabular_df.dataloaders(bs=64, drop_last=True)\n",
    "data_loader.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Define the metrics of choice and the tabular learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = Recall()\n",
    "precision = Precision()\n",
    "learn = tabular_learner(\n",
    "    data_loader, \n",
    "    [500, 200], \n",
    "    metrics=[accuracy, recall, precision]\n",
    ")\n",
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also figure out the embeddings using the following snippet\n",
    "emb_szs = get_emb_sz(tabular_df)\n",
    "emb_szs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Embedding(11, 6)` means that a categorical embedding was created with 11 input values and 6 output latent features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Find the suggested learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T13:25:54.778861Z",
     "start_time": "2020-01-17T13:25:45.768432Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "\n",
    "# plt.savefig(\"images/figure_15_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Train the Tabular learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T13:30:25.065068Z",
     "start_time": "2020-01-17T13:25:55.120308Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.fit(n_epoch=25, lr=1e-3, wd=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T23:55:21.268728Z",
     "start_time": "2019-12-23T23:55:21.263553Z"
    }
   },
   "source": [
    "10. Plot the losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T13:30:31.196524Z",
     "start_time": "2020-01-17T13:30:25.374563Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Define the validation `DataLoaders`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_loader = learn.dls.test_dl(df.loc[list(splits[1])])\n",
    "valid_data_loader.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Evaluate the performance on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.validate(dl=valid_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Get predictions for the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, y_true = learn.get_preds(dl=valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Inspect the performance evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T13:30:39.983925Z",
     "start_time": "2020-01-17T13:30:39.672528Z"
    }
   },
   "outputs": [],
   "source": [
    "perf = performance_evaluation_report_fastai(\n",
    "    learn, valid_data_loader, show_plot=True\n",
    ")\n",
    "\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_6\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also be more specific when creating the training/validation split. Below, we use the `sklearn` funcitonalities and pass indices to the `IndexSplitter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T22:24:58.938321Z",
     "start_time": "2020-01-14T22:24:56.101Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop(TARGET)\n",
    "\n",
    "strat_split = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_ind, test_ind = next(strat_split.split(X, y))\n",
    "ind_splits = IndexSplitter(valid_idx=list(test_ind))(range_of(df))\n",
    "\n",
    "tabular_df = TabularPandas(\n",
    "    df, \n",
    "    procs=preprocessing,\n",
    "    cat_names=cat_features,\n",
    "    cont_names=num_features,\n",
    "    y_names=TARGET,\n",
    "    y_block=CategoryBlock(),\n",
    "    splits=ind_splits\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look into the example results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or create predictions for a single row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row, clas, probs = learn.predict(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2 Exploring Google's TabNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T13:25:43.412338Z",
     "start_time": "2020-01-17T13:25:41.274701Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T23:47:19.038850Z",
     "start_time": "2019-12-23T23:47:19.036355Z"
    }
   },
   "source": [
    "2. Load the dataset from a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T13:25:43.675235Z",
     "start_time": "2020-01-17T13:25:43.535484Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/credit_card_default.csv\", \n",
    "                 na_values=\"\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Separate the target from the features and create lists with numerical/categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(\"default_payment_next_month\")\n",
    "\n",
    "cat_features = list(X.select_dtypes(\"object\").columns)\n",
    "num_features = list(X.select_dtypes(\"number\").columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a reminder, where the possible missing values are\n",
    "X.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Impute missing values in the categorical features, encode them using `LabelEncoder` and store the number of unique categories per feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dims = {}\n",
    "\n",
    "for col in cat_features:\n",
    "    label_encoder = LabelEncoder()\n",
    "    X[col] = X[col].fillna(\"Missing\")\n",
    "    X[col] = label_encoder.fit_transform(X[col].values)\n",
    "    cat_dims[col] = len(label_encoder.classes_)\n",
    "\n",
    "cat_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create a train/valid/test split using the 70-15-15 ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the initial split - training and temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# create the valid and test sets\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    stratify=y_temp, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of data in each set ----\")\n",
    "print(f\"Train: {100 * len(X_train) / len(X):.2f}%\")\n",
    "print(f\"Valid: {100 * len(X_valid) / len(X):.2f}%\")\n",
    "print(f\"Test: {100 * len(X_test) / len(X):.2f}%\")\n",
    "print(\"\")\n",
    "print(\"Class distribution in each set ----\")\n",
    "print(f\"Train: {y_train.value_counts(normalize=True).values}\")\n",
    "print(f\"Valid: {y_valid.value_counts(normalize=True).values}\")\n",
    "print(f\"Test: {y_test.value_counts(normalize=True).values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Impute the missing values in the numerical features across all the sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_features:\n",
    "    imp_mean = X_train[col].mean()\n",
    "    X_train[col] = X_train[col].fillna(imp_mean)\n",
    "    X_valid[col] = X_valid[col].fillna(imp_mean)\n",
    "    X_test[col] = X_test[col].fillna(imp_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Prepare lists with the indices of categorical features and the number of unique categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X.columns.to_list()\n",
    "cat_ind = [features.index(feat) for feat in cat_features] \n",
    "cat_dims = list(cat_dims.values())\n",
    "cat_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Define a custom recall metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recall(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"recall\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        y_pred = np.argmax(y_score, axis=1)\n",
    "        return recall_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Define TabNet's parameters and instantiate the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_params = {\n",
    "    \"cat_idxs\": cat_ind,\n",
    "    \"cat_dims\": cat_dims,\n",
    "    \"optimizer_fn\": torch.optim.Adam,\n",
    "    \"optimizer_params\": dict(lr=2e-2),\n",
    "    \"scheduler_params\": {\n",
    "        \"step_size\":20,\n",
    "        \"gamma\":0.9\n",
    "    },\n",
    "    \"scheduler_fn\": torch.optim.lr_scheduler.StepLR,\n",
    "    \"mask_type\": \"sparsemax\",\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "tabnet = TabNetClassifier(**tabnet_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Train the TabNet classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet.fit(\n",
    "    X_train=X_train.values, \n",
    "    y_train=y_train.values,\n",
    "    eval_set=[\n",
    "        (X_train.values, y_train.values), \n",
    "        (X_valid.values, y_valid.values)\n",
    "    ],\n",
    "    eval_name=[\"train\", \"valid\"],\n",
    "    eval_metric=[\"auc\", Recall],\n",
    "    max_epochs=200, \n",
    "    patience=20,\n",
    "    batch_size=1024, \n",
    "    virtual_batch_size=128,\n",
    "    weights=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Prepare the history DataFrame and plot the scores over epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(tabnet.history.history)\n",
    "history_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df[\"loss\"].plot(\n",
    "    title=\"Loss over epochs\",\n",
    "    xlabel=\"epochs\",\n",
    "    ylabel=\"loss\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    history_df[[\"train_auc\", \"valid_auc\"]]\n",
    "    .plot(title=\"AUC over epochs\",\n",
    "          xlabel=\"epochs\",\n",
    "          ylabel=\"AUC\")\n",
    ");\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    history_df[[\"train_recall\", \"valid_recall\"]]\n",
    "    .plot(title=\"Recall over epochs\",\n",
    "          xlabel=\"epochs\",\n",
    "          ylabel=\"recall\")\n",
    ");\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Create predictions for the test set and evaluate their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tabnet.predict(X_test.values)\n",
    "\n",
    "print(f\"Best validation score: {tabnet.best_cost:.4f}\")\n",
    "print(f\"Test set score: {recall_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Extract and plot the global feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_feat_imp = pd.Series(tabnet.feature_importances_, \n",
    "                            index=X_train.columns)\n",
    "(\n",
    "    tabnet_feat_imp\n",
    "    .nlargest(20)\n",
    "    .sort_values()\n",
    "    .plot(kind=\"barh\",\n",
    "          title=\"TabNet's feature importances\")\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(tabnet.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_matrix, masks = tabnet.explain(X_test.values)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].imshow(masks[i][:50])\n",
    "    axs[i].set_title(f\"mask {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tabnet model\n",
    "MODEL_PATH = \"models/tabnet_model\"\n",
    "saved_filepath = tabnet.save_model(MODEL_PATH)\n",
    "\n",
    "# define new model with basic parameters and load state dict weights\n",
    "loaded_tabnet = TabNetClassifier()\n",
    "loaded_tabnet.load_model(saved_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3 Time series forecasting with Amazon's DeepAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yfinance as yf\n",
    "from random import sample, seed\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_forecasting import DeepAR, TimeSeriesDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Download the tickers of the SP500 constituents and sample 100 random tickers from the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_html(\n",
    "    \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    ")\n",
    "df = df[0]\n",
    "\n",
    "seed(44)\n",
    "sampled_tickers = sample(df[\"Symbol\"].to_list(), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Download the historical stock prices of the selected stocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = yf.download(sampled_tickers,\n",
    "                     start=\"2020-01-01\",\n",
    "                     end=\"2021-12-31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Keep the adjusted close price and remove the stocks with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df[\"Adj Close\"]\n",
    "df = df.loc[:, ~df.isna().any()]\n",
    "selected_tickers = df.columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Convert the data's format from wide to long and add the time index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=False)\n",
    "df = (\n",
    "    pd.melt(df, \n",
    "            id_vars=[\"Date\"], \n",
    "            value_vars=selected_tickers, \n",
    "            value_name=\"price\"\n",
    "    ).rename(columns={\"variable\": \"ticker\"})\n",
    ")\n",
    "df[\"time_idx\"] = df.groupby(\"ticker\").cumcount()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define constants used for setting up the model's training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ENCODER_LENGTH = 40\n",
    "MAX_PRED_LENGTH = 20\n",
    "BATCH_SIZE = 128\n",
    "MAX_EPOCHS = 30\n",
    "training_cutoff = df[\"time_idx\"].max() - MAX_PRED_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Define the training and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TimeSeriesDataSet(\n",
    "    df[lambda x: x[\"time_idx\"] <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"price\",\n",
    "    group_ids=[\"ticker\"],\n",
    "    time_varying_unknown_reals=[\"price\"],\n",
    "    max_encoder_length=MAX_ENCODER_LENGTH,\n",
    "    max_prediction_length=MAX_PRED_LENGTH,\n",
    ")\n",
    "\n",
    "valid_set = TimeSeriesDataSet.from_dataset(\n",
    "    train_set, df, min_prediction_idx=training_cutoff+1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Get the DataLoaders from the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = train_set.to_dataloader(\n",
    "    train=True, batch_size=BATCH_SIZE\n",
    ")\n",
    "valid_dataloader = valid_set.to_dataloader(\n",
    "    train=False, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Define the DeepAR model and find the suggested learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "\n",
    "deep_ar = DeepAR.from_dataset(\n",
    "    train_set, \n",
    "    learning_rate=1e-2,\n",
    "    hidden_size=30, \n",
    "    rnn_layers=4\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(gradient_clip_val=1e-1)\n",
    "res = trainer.tuner.lr_find(\n",
    "    deep_ar,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader,\n",
    "    min_lr=1e-5,\n",
    "    max_lr=1e0,\n",
    "    early_stop_threshold=100,\n",
    ")\n",
    "\n",
    "print(f\"Suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Train the DeepAR model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "\n",
    "deep_ar.hparams.learning_rate = res.suggestion()\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    min_delta=1e-4, \n",
    "    patience=10\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    deep_ar,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Extract the best DeepAR model from a checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = DeepAR.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Create the predictions for the validation set and plot 5 of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions, x = best_model.predict(\n",
    "    valid_dataloader, \n",
    "    mode=\"raw\", \n",
    "    return_x=True, \n",
    "    n_samples=100\n",
    ")\n",
    "\n",
    "tickers = valid_set.x_to_index(x)[\"ticker\"]\n",
    "\n",
    "for idx in range(5):\n",
    "    best_model.plot_prediction(\n",
    "        x, raw_predictions, idx=idx, add_loss_to_title=True\n",
    "    )\n",
    "    plt.suptitle(f\"Ticker: {tickers.iloc[idx]}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    # plt.savefig(f\"images/figure_15_12_{idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.metrics import MultivariateNormalDistributionLoss\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = generate_ar_data(\n",
    "#     seasonality=10.0, \n",
    "#     timesteps=len(raw_df), \n",
    "#     n_series=len(selected_tickers), \n",
    "#     seed=42\n",
    "# )\n",
    "# df[\"date\"] = pd.Timestamp(\"2020-01-01\") + pd.to_timedelta(df.time_idx, \"D\")\n",
    "# df = df.astype(dict(series=str))\n",
    "# df.columns = [\"ticker\", \"time_idx\", \"price\", \"date\"]\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the DataLoaders again, this time specifying the `batch_sampler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TimeSeriesDataSet(\n",
    "    df[lambda x: x[\"time_idx\"] <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"price\",\n",
    "    group_ids=[\"ticker\"],\n",
    "    static_categoricals=[\"ticker\"],  \n",
    "    time_varying_unknown_reals=[\"price\"],\n",
    "    max_encoder_length=MAX_ENCODER_LENGTH,\n",
    "    max_prediction_length=MAX_PRED_LENGTH,\n",
    ")\n",
    "\n",
    "valid_set = TimeSeriesDataSet.from_dataset(\n",
    "    train_set, df, min_prediction_idx=training_cutoff+1\n",
    ")\n",
    "\n",
    "train_dataloader = train_set.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_sampler=\"synchronized\"\n",
    ")\n",
    "valid_dataloader = valid_set.to_dataloader(\n",
    "    train=False, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    batch_sampler=\"synchronized\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define the DeepVAR model and find the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "\n",
    "deep_var = DeepAR.from_dataset(\n",
    "    train_set, \n",
    "    learning_rate=1e-2, \n",
    "    hidden_size=30, \n",
    "    rnn_layers=4,\n",
    "    loss=MultivariateNormalDistributionLoss()\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(gradient_clip_val=1e-1)\n",
    "res = trainer.tuner.lr_find(\n",
    "    deep_var,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader,\n",
    "    min_lr=1e-5,\n",
    "    max_lr=1e0,\n",
    "    early_stop_threshold=100,\n",
    ")\n",
    "\n",
    "print(f\"Suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train the DeepVAR model using the selected learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "\n",
    "deep_var.hparams.learning_rate = res.suggestion()\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    min_delta=1e-4, \n",
    "    patience=10\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    deep_var,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Extract the best DeepVAR model from a checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = DeepAR.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create the predictions for the validation set and plot 5 of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions, x = best_model.predict(\n",
    "    valid_dataloader, \n",
    "    mode=\"raw\", \n",
    "    return_x=True, \n",
    "    n_samples=100\n",
    ")\n",
    "tickers = valid_set.x_to_index(x)[\"ticker\"]\n",
    "\n",
    "for idx in range(5):\n",
    "    best_model.plot_prediction(\n",
    "        x, raw_predictions, idx=idx, add_loss_to_title=True\n",
    "    )\n",
    "    plt.suptitle(f\"Ticker: {tickers.iloc[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Extract the correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = best_model.predict(valid_dataloader, \n",
    "                           mode=(\"raw\", \"prediction\"), \n",
    "                           n_samples=None)\n",
    "                           \n",
    "cov_matrix = (\n",
    "    best_model\n",
    "    .loss\n",
    "    .map_x_to_distribution(preds)\n",
    "    .base_dist\n",
    "    .covariance_matrix\n",
    "    .mean(0)\n",
    ")\n",
    "\n",
    "# normalize the covariance matrix diagonal to 1.0\n",
    "cov_diag_mult = (\n",
    "    torch.diag(cov_matrix)[None] * torch.diag(cov_matrix)[None].T\n",
    ")\n",
    "corr_matrix = cov_matrix / torch.sqrt(cov_diag_mult)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Plot the correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "fif, ax = plt.subplots()\n",
    "\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix, mask=mask, cmap=cmap, \n",
    "    vmax=.3, center=0, square=True, \n",
    "    linewidths=.5, cbar_kws={\"shrink\": .5}\n",
    ")\n",
    "\n",
    "ax.set_title(\"Correlation matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of off-diagonal correlations\n",
    "plt.hist(corr_matrix[corr_matrix < 1].numpy())\n",
    "\n",
    "plt.xlabel(\"Correlation\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.4 Time series forecasting with NeuralProphet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from neuralprophet import NeuralProphet\n",
    "from neuralprophet.utils import set_random_seed\n",
    "from neuralprophet.utils import set_log_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Download the historical prices of the S&P 500 index and prepare the DataFrame for modeling with NeuralProphet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yf.download(\"^GSPC\",\n",
    "                 start=\"2010-01-01\",\n",
    "                 end=\"2021-12-31\")\n",
    "df = df[[\"Adj Close\"]].reset_index(drop=False)\n",
    "df.columns = [\"ds\", \"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create the train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_LENGTH = 60\n",
    "df_train = df.iloc[:-TEST_LENGTH]\n",
    "df_test = df.iloc[-TEST_LENGTH:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train the default Prophet model and plot the evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import xlabel\n",
    "\n",
    "\n",
    "set_random_seed(42)\n",
    "set_log_level(log_level=\"ERROR\")\n",
    "model = NeuralProphet(changepoints_range=0.95)\n",
    "metrics = model.fit(df_train, freq=\"B\")\n",
    "\n",
    "(\n",
    "    metrics\n",
    "    .drop(columns=[\"RegLoss\"])\n",
    "    .plot(title=\"Evaluation metrics during training\", \n",
    "          subplots=True,\n",
    "          xlabel=\"epochs\",\n",
    "          ylabel=\"metric\")\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Calculate the predictions and plot the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = model.predict(df)\n",
    "\n",
    "pred_df.plot(x=\"ds\", y=[\"y\", \"yhat1\"], \n",
    "             title=\"S&P 500 - forecast vs ground truth\",\n",
    "             ylabel=\"value\");\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pred_df\n",
    "    .iloc[-TEST_LENGTH:]\n",
    "    .plot(x=\"ds\", y=[\"y\", \"yhat1\"], \n",
    "          title=\"S&P 500 - forecast vs ground truth\",\n",
    "          ylabel=\"value\")\n",
    ");\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Add the AR components to NeuralProphet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(42)\n",
    "set_log_level(log_level=\"ERROR\")\n",
    "model = NeuralProphet(\n",
    "    changepoints_range=0.95,\n",
    "    n_lags=10,\n",
    "    ar_reg=1,\n",
    ")\n",
    "metrics = model.fit(df_train, freq=\"B\")\n",
    "\n",
    "pred_df = model.predict(df)\n",
    "pred_df.plot(x=\"ds\", y=[\"y\", \"yhat1\"], \n",
    "             title=\"S&P 500 - forecast vs ground truth\",\n",
    "             ylabel=\"value\");\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pred_df\n",
    "    .iloc[-TEST_LENGTH:]\n",
    "    .plot(x=\"ds\", y=[\"y\", \"yhat1\"], \n",
    "          title=\"S&P 500 - forecast vs ground truth\",\n",
    "          ylabel=\"value\")\n",
    ");\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Add the AR-Net to NeuralProphet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(42)\n",
    "set_log_level(log_level=\"ERROR\")\n",
    "model = NeuralProphet(\n",
    "    changepoints_range=0.95,\n",
    "    n_lags=10,\n",
    "    ar_reg=1,\n",
    "    num_hidden_layers=3,\n",
    "    d_hidden=32,\n",
    ")\n",
    "metrics = model.fit(df_train, freq=\"B\")\n",
    "\n",
    "pred_df = model.predict(df)\n",
    "\n",
    "(\n",
    "    pred_df\n",
    "    .iloc[-TEST_LENGTH:]\n",
    "    .plot(x=\"ds\", y=[\"y\", \"yhat1\"], \n",
    "          title=\"S&P 500 - forecast vs ground truth\",\n",
    "          ylabel=\"value\")\n",
    ");\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Plot the components and parameters of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting only, as there is some issue with the AR plot\n",
    "# after plotting the components we can revert to the settings at the top of the Notebook\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_components(model.predict(df_train));\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_parameters();\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_23\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add holidays to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(42)\n",
    "set_log_level(log_level=\"ERROR\")\n",
    "model = NeuralProphet(\n",
    "    changepoints_range=0.95,\n",
    "    n_lags=10,\n",
    "    ar_reg=1,\n",
    "    num_hidden_layers=3,\n",
    "    d_hidden=32,\n",
    ")\n",
    "\n",
    "model = model.add_country_holidays(\n",
    "    \"US\", lower_window=-1, upper_window=1\n",
    ")\n",
    "metrics = model.fit(df_train, freq=\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = model.predict(df_train)\n",
    "model.plot_components(pred_df)\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_parameters();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a 10-day ahead multi-step forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(42)\n",
    "set_log_level(log_level=\"ERROR\")\n",
    "model = NeuralProphet(\n",
    "    n_lags=10,\n",
    "    n_forecasts=10,\n",
    "    ar_reg=1,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "metrics = model.fit(df_train, freq=\"B\")\n",
    "pred_df = model.predict(df)\n",
    "pred_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_random_seed(42)\n",
    "pred_df = model.predict(df, raw=True, decompose=False)\n",
    "pred_df.tail().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = model.predict(df_test)\n",
    "model.plot(pred_df)\n",
    "ax = plt.gca()\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "ax.set_title(\"10-day ahead multi-step forecast\")\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_27\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.highlight_nth_step_ahead_of_each_forecast(1)\n",
    "model.plot(pred_df)\n",
    "ax = plt.gca()\n",
    "ax.set_title(\"Step 1 of the 10-day ahead multi-step forecast\")\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# plt.savefig(\"images/figure_15_28\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "0117835dafdb051235b33d006a7ad155411608685e1d44af6fb551f6db3e7774"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
